import streamlit as st
from PIL import Image
import numpy as np
import cv2
import os
import sys
from pathlib import Path

# Ensure project root and src/ are on sys.path (works before and after flattening)
def _discover_project_root_and_src():
    here = Path(__file__).resolve()
    src_dir = None
    for anc in here.parents:
        if anc.name == "src":
            src_dir = anc
            project_root = anc.parent
            break
    if src_dir is None:
        # Fallback to original layout assumption
        project_root = here.parents[1]
        src_dir = project_root / "src"
    return project_root, src_dir

PROJECT_ROOT, SRC_DIR = _discover_project_root_and_src()
for p in (PROJECT_ROOT, SRC_DIR):
    if str(p) not in sys.path:
        sys.path.insert(0, str(p))

# Import from src.<module>
from src.infer import InferenceModel
from src.tips_llm import generate_tips
from src.gemini import analyze_plant_with_gemini
# YOLO fully removed; no detector UI or messaging

st.set_page_config(page_title="Multi-Crop Recognition", layout="wide")

# Theming & animated background
import streamlit as st

# Enhanced professional green theme style for Streamlit
professional_green_theme = """
<style>
/* Root color variables */
:root {
  --primary: #1b5e20;
  --accent: #4caf50;
  --bg-start: #0c1a0c;
  --bg-end: #142914;
  --card-bg: #1e3c16;
  --text-color: #e3f2df;
  --header-color: #a9d18e;
  --button-bg: #4caf50;
  --button-hover-bg: #6bbf59;
  --shadow-color: rgba(27, 94, 32, 0.4);
}

/* Overall app background and text */
.stApp {
  background: linear-gradient(135deg, var(--bg-start), var(--bg-end));
  color: var(--text-color);
  font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
  line-height: 1.6;
  padding: 1rem;
}

/* Animated soft gradient overlay */
.bg-anim {
  position: fixed;
  top: 0; left: 0;
  width: 100%; height: 100%;
  pointer-events: none;
  z-index: -1;
  background:
    radial-gradient(1200px 600px at 10% 20%, rgba(76,175,80,0.1), transparent),
    radial-gradient(900px 500px at 90% 10%, rgba(27,94,32,0.1), transparent),
    radial-gradient(1000px 700px at 50% 90%, rgba(56,142,60,0.08), transparent);
  animation: floatbg 15s ease-in-out infinite alternate;
}
@keyframes floatbg {
  from { transform: translateY(-8px); }
  to { transform: translateY(8px); }
}

/* Sidebar style */
div[data-testid="stSidebar"] {
  background-color: var(--card-bg);
  box-shadow: 2px 0 8px var(--shadow-color);
  border-radius: 0 10px 10px 0;
  padding: 1rem 1.5rem;
}

/* Text elements */
.stMarkdown, .stText, .stAlert, .stDataFrame {
  color: var(--text-color);
  font-weight: 400;
}

/* Headers with subtle shadows */
h1, h2, h3 {
  color: var(--header-color);
  text-shadow: 1px 1px 3px var(--shadow-color);
  margin-bottom: 0.5rem;
}

/* Buttons with smooth transitions and shadows */
button[kind="primary"], .stButton>button {
  background-color: var(--button-bg);
  color: #0b150c;
  border: none;
  padding: 0.5rem 1.2rem;
  border-radius: 5px;
  font-weight: 600;
  box-shadow: 0 3px 8px var(--shadow-color);
  transition: background-color 0.3s ease, box-shadow 0.3s ease;
  cursor: pointer;
}
button[kind="primary"]:hover, .stButton>button:hover {
  background-color: var(--button-hover-bg);
  box-shadow: 0 5px 15px var(--shadow-color);
}

/* Inputs and text areas styling */
input, textarea, select {
  background-color: var(--card-bg);
  border: 1px solid #2f5d2f;
  color: var(--text-color);
  border-radius: 5px;
  padding: 0.4rem 0.7rem;
  font-size: 1rem;
  transition: border-color 0.3s ease;
}
input:focus, textarea:focus, select:focus {
  border-color: var(--accent);
  outline: none;
  box-shadow: 0 0 8px var(--accent);
}

/* Tables styling */
.stDataFrame table {
  border-collapse: collapse;
  width: 100%;
  background: var(--card-bg);
  color: var(--text-color);
  border-radius: 8px;
  overflow: hidden;
}
.stDataFrame th, .stDataFrame td {
  padding: 0.5rem 1rem;
  border-bottom: 1px solid #2f5d2f;
  text-align: left;
}
.stDataFrame th {
  background-color: var(--primary);
  color: #d4eecd;
}

/* Hide default Streamlit footer */
footer {
  visibility: hidden;
  height: 0;
  margin: 0;
  padding: 0;
}
</style>
<div class="bg-anim"></div>
"""

st.markdown(professional_green_theme, unsafe_allow_html=True)

@st.cache_resource
def load_model():
    # Use model name stored in checkpoint cfg; do not override here
    return InferenceModel("weights/best.pt", model_name=None, device=None)

model = load_model()

st.title("ðŸŒ± Multi-Crop Plant Recognition")
from src import gemini as _gemini

# Detect Gemini keys silently; if none, hide Gemini section
try:
    _keys = _gemini._get_all_keys(None)
except Exception:
    _keys = []
HAS_GEMINI = bool(_keys)
analysis_mode = st.selectbox(
    "Analysis mode",
    options=["Fast (3â€“5s)", "Detailed (8â€“12s)"],
    index=1,
    help="Fast mode returns concise guidance quickly. Detailed mode provides longer analysis."
)
left, right = st.columns([2,1])
with left:
    uploaded = st.file_uploader("Upload a plant image", type=["jpg","jpeg","png"])
with right:
    cam = st.camera_input("Or capture from camera")

img = None
if uploaded is not None:
    img = Image.open(uploaded).convert("RGB")
elif cam is not None:
    img = Image.open(cam).convert("RGB")

if img:
    # Show input image as requested
    st.image(img, caption="Input", use_column_width=True)
    rgb = np.array(img)
    # Full-image classification (YOLO removed)
    crop_rgb = rgb
    context_note = ""
    # Use multi-crop for robustness (works without YOLO training too)
    preds = model.predict_multicrop(crop_rgb, topk=5, tta="fast")
    top1_class, top1_conf = preds[0]  # Get first prediction (class, confidence)
    # If low confidence, retry with stronger TTA to reduce false predictions
    if top1_conf < 0.35:
        preds = model.predict_multicrop(crop_rgb, topk=5, tta="full")
        top1_class, top1_conf = preds[0]
    # With YOLO disabled, skip fruit-label-specific heuristics

    # Gemini analysis section (only if key available)
    if HAS_GEMINI:
        st.divider()
        st.subheader("Gemini Disease Analysis & Remedies")
        try:
            # Adaptive hand-off: compute margin and entropy to decide how to brief Gemini
            probs = np.array([p for _, p in preds], dtype=float)
            p1 = float(probs[0]) if len(probs) else 0.0
            p2 = float(probs[1]) if len(probs) > 1 else 0.0
            margin = p1 - p2
            # normalized entropy (0..1)
            eps = 1e-8
            norm = probs.sum() + eps
            q = probs / norm
            entropy = float(-(q * np.log(q + eps)).sum() / (np.log(len(q) + eps))) if len(q) > 1 else 0.0

            # Decide confidence bucket and build candidate_str
            # High confidence if p1>=0.70 and margin>=0.20 and entropy<=0.65
            # Medium if p1>=0.50 or margin>=0.10
            # Low otherwise
            if (p1 >= 0.70 and margin >= 0.20 and entropy <= 0.65):
                conf_bucket = "High"
                candidate_str = top1_class
            elif (p1 >= 0.50 or margin >= 0.10):
                conf_bucket = "Medium"
                k = min(3, len(preds))
                topk = ", ".join([f"{c} ({p:.2f})" for c, p in preds[:k]])
                candidate_str = f"Candidates: {topk}"
            else:
                conf_bucket = "Low"
                k = min(5, len(preds))
                topk = ", ".join([f"{c} ({p:.2f})" for c, p in preds[:k]])
                candidate_str = f"Uncertain; please identify species from image. Candidate list: {topk}"

            st.caption(f"Model confidence: {conf_bucket} | p1={p1:.2f}, margin={margin:.2f}, entropy={entropy:.2f}")
            # Look-alike nudge: cucurbits vs brassicas to reduce confusion
            cucurbits = {"Pumpkins, squash and gourds plant", "Cucumber plant", "Zucchini plant", "Pumpkin plant", "Squash plant"}
            brassicas = {"Cabbages and other brassicas plant", "Mustard greens plant", "Rapeseed (Canola) plant", "Brussels sprout plant"}
            top_names = {c for c,_ in preds[:5]}
            if top_names & cucurbits and (top_names & brassicas):
                candidate_str += ", Possible species group: cucurbits (pumpkin/squash/cucumber)"
            # Parameters based on mode
            if analysis_mode.startswith("Fast"):
                timeout_sec = 4.0
                max_tokens = 320
                max_side = 640
                spinner_msg = "Analyzing with Gemini (fast mode)..."
                model_name = None  # use default fast model
            else:
                timeout_sec = 12.0
                max_tokens = 1200
                max_side = 1024
                spinner_msg = "Analyzing with Gemini (detailed mode)..."
                model_name = "gemini-1.5-pro"

            with st.spinner(spinner_msg):
                # If we cropped a region for fruit context, send that crop to Gemini for visual grounding
                img_for_llm = Image.fromarray(crop_rgb)
                species_text = candidate_str if not context_note else f"{context_note}. {candidate_str}"
                result = analyze_plant_with_gemini(
                    img_for_llm,
                    species_text,
                    top1_conf,
                    api_key=None,
                    timeout_sec=timeout_sec,
                    max_output_tokens=max_tokens,
                    max_side=max_side,
                    temperature=0.7 if analysis_mode.startswith("Detailed") else 0.6,
                    model_name=model_name,
                )
            content = result.get("gemini_output", "No response")
            # Language toggle and sectioned display
            st.divider()
            lang = st.radio("Language / à¤­à¤¾à¤·à¤¾", ["English", "Hindi"], index=0, horizontal=True)

            def extract_section(text: str, header: str) -> str:
                import re
                # Capture from header to next top-level header or end
                pattern = rf"(?s)###\s+{re.escape(header)}\s*(.*?)(?=\n###\s+|\Z)"
                m = re.search(pattern, text)
                return m.group(1).strip() if m else ""

            eng = extract_section(content, "English")
            hin = extract_section(content, "Hindi")
            if lang == "English" and eng:
                st.markdown("### English\n" + eng)
            elif lang == "Hindi" and hin:
                st.markdown("### Hindi\n" + hin)
            else:
                # Fallback: show full content if parsing failed
                st.markdown(content)
            if "latency_sec" in result:
                st.caption(f"Gemini latency: {result['latency_sec']}s | Mode: {analysis_mode}")
                # Auto-upgrade: if fast mode produced fallback or too-short output, run a detailed retry automatically
                if analysis_mode.startswith("Fast") and ("Response timed out" in content or len(content) < 300):
                    with st.spinner("Generating detailed analysis (one-time upgrade)..."):
                        result2 = analyze_plant_with_gemini(
                            img,
                            top1_class,
                            top1_conf,
                            api_key=None,
                            timeout_sec=9.0,
                            max_output_tokens=900,
                            max_side=896,
                            temperature=0.7,
                        )
                    st.markdown("---")
                    st.subheader("Detailed Analysis")
                    st.markdown(result2.get("gemini_output", "No response"))
                    if "latency_sec" in result2:
                        st.caption(f"Detailed latency: {result2['latency_sec']}s")
                # Detailed retry: if Detailed mode still times out or is too short, force a single extended retry
                if analysis_mode.startswith("Detailed") and ("Response timed out" in content or len(content) < 600):
                    with st.spinner("Retrying with extended detailed mode..."):
                        result3 = analyze_plant_with_gemini(
                            img,
                            top1_class,
                            top1_conf,
                            api_key=None,
                            timeout_sec=18.0,
                            max_output_tokens=1600,
                            max_side=1280,
                            temperature=0.7,
                            model_name="gemini-1.5-pro",
                        )
                    st.markdown("---")
                    st.subheader("Extended Detailed Analysis")
                    st.markdown(result3.get("gemini_output", "No response"))
                    if "latency_sec" in result3:
                        st.caption(f"Extended detailed latency: {result3['latency_sec']}s")
        except Exception as e:
            st.error(f"Gemini analysis failed: {e}")
